{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Word-splitting and text segmentation in East Asian languages\n",
                "\n",
                "As different as they are, Chinese, Japanese and Korean are lumped together as [CJK languages](https://en.wikipedia.org/wiki/CJK_characters) when discussed from an English-language point of view. One reason they're considered similar is that **spacing** is not used in the same way as in English. While analyzing English requires splitting sentences based on spaces, one difficulty for this language set (and others) is **determining where the breaks between words are.** In this section we will review how to use libraries to segment words in these languages as well as Thai and Vietnamese."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<p class=\"reading-options\">\n  <a class=\"btn\" href=\"/text-analysis/splitting-words-in-east-asian-languages\">\n    <i class=\"fa fa-sm fa-book\"></i>\n    Read online\n  </a>\n  <a class=\"btn\" href=\"/text-analysis/notebooks/Splitting words in East Asian languages.ipynb\">\n    <i class=\"fa fa-sm fa-download\"></i>\n    Download notebook\n  </a>\n  <a class=\"btn\" href=\"https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/Splitting words in East Asian languages.ipynb\" target=\"_new\">\n    <i class=\"fa fa-sm fa-laptop\"></i>\n    Interactive version\n  </a>\n</p>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using libraries\n",
                "\n",
                "**Segmenting** is the process of splitting a text into separate words. There isn't always a \"right\" answer as to what the split should be, so you might have to **try a few different libraries** before you feel a good fit. The recommendations below aren't necessarily the best Python packages, they're just ones that had a bit of activity, seemingly-decent interfaces/documentation, and no external installs.\n",
                "\n",
                "### Using these libraries with scikit-learn\n",
                "\n",
                "After reading this page, you might want to learn how to use these libraries with scikit-learn vectorizers. In that case, check out tutorial on [how to make scikit-learn vectorizers work with Japanese, Chinese, and other East Asian languages page](/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/) after you read this page.\n",
                "\n",
                "## Chinese: jieba\n",
                "\n",
                "The Chinese word segmentation library [jieba](https://github.com/fxsjy/jieba) is very popular when analyzing Chinese text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install jieba"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Jieba has a few different techniques we can use to segment words. We can read the documentation to get into details, but one major question is whether we want the **smallest possible divisions.**\n",
                "\n",
                "Using `lcut` gives us individual words and what might be considered _noun phrases_."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\u6211', '\u6765\u5230', '\u5317\u4eac', '\u6e05\u534e\u5927\u5b66']"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import jieba\n",
                "\n",
                "jieba.lcut('\u6211\u6765\u5230\u5317\u4eac\u6e05\u534e\u5927\u5b66')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If we want to divide things up a bit more, we can add `cut_all=True`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\u6211', '\u6765\u5230', '\u5317\u4eac', '\u6e05\u534e', '\u6e05\u534e\u5927\u5b66', '\u534e\u5927', '\u5927\u5b66']"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "jieba.lcut('\u6211\u6765\u5230\u5317\u4eac\u6e05\u534e\u5927\u5b66', cut_all=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The big difference is `cut_all` will split something like `\u6e05\u534e\u5927\u5b66` into both `\u6e05\u534e` and `\u534e\u5927`.\n",
                "\n",
                "When might we use one compared to the other?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Japanese: nagisa\n",
                "\n",
                "For Japanese, you'll be using the library [nagisa](https://github.com/taishi-i/nagisa)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install nagisa"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['Python', '\u3067', '\u7c21\u5358', '\u306b', '\u4f7f\u3048\u308b', '\u30c4\u30fc\u30eb', '\u3067\u3059']"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import nagisa\n",
                "\n",
                "text = 'Python\u3067\u7c21\u5358\u306b\u4f7f\u3048\u308b\u30c4\u30fc\u30eb\u3067\u3059'\n",
                "doc = nagisa.tagging(text)\n",
                "\n",
                "doc.words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In addition to simple tokenization, nagisa will also do **part-of-speech tagging**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\u540d\u8a5e', '\u52a9\u8a5e', '\u5f62\u72b6\u8a5e', '\u52a9\u52d5\u8a5e', '\u52d5\u8a5e', '\u540d\u8a5e', '\u52a9\u52d5\u8a5e']"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "words.postags"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This allows you to do things like pluck out all of the nouns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['Python', '\u30c4\u30fc\u30eb']"
                        ]
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "nouns = nagisa.extract(text, extract_postags=['\u540d\u8a5e'])\n",
                "nouns.words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Korean: KoNLPy\n",
                "\n",
                "Korean does use spaces, but certain characters combine with the \"actual\" words, so you can't just split on spaces. [KoNPLy](http://konlpy.org/) has several engines that will help you with this. [See a comparison chart of the different engines here](http://konlpy.org/en/latest/morph/#performance-analysis) or [find more specific details](https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install konlpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "phrase = \"\uc544\ubc84\uc9c0\uac00\ubc29\uc5d0\ub4e4\uc5b4\uac00\uc2e0\ub2e4\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\uc544\ubc84\uc9c0\uac00\ubc29\uc5d0\ub4e4\uc5b4\uac00', '\uc774', '\uc2dc\u3134\ub2e4']"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from konlpy.tag import Hannanum\n",
                "hannanum = Hannanum()\n",
                "hannanum.morphs(phrase)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\uc544\ubc84\uc9c0', '\uac00\ubc29', '\uc5d0', '\ub4e4\uc5b4\uac00', '\uc2dc', '\u3134\ub2e4']"
                        ]
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from konlpy.tag import Kkma\n",
                "kkma = Kkma()\n",
                "kkma.morphs(phrase)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\uc544\ubc84\uc9c0', '\uac00\ubc29', '\uc5d0', '\ub4e4\uc5b4\uac00', '\uc2dc', '\u3134\ub2e4']"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from konlpy.tag import Komoran\n",
                "komoran = Komoran()\n",
                "komoran.morphs(phrase)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Thai: tltk\n",
                "\n",
                "You can find [many Thai NLP packages here](https://github.com/kobkrit/nlp_thai_resources), but we'll focus on [tltk](https://pypi.org/project/tltk/). It doesn't have the best documentation and it might not be the most accurate, but it **doesn't require us to install anything extra** (e.g. Tensorflow) and that's the absolutely only reason why we're using it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install tltk"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[[('\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19', 'NOUN'),\n",
                            "  ('\u0e40\u0e02\u0e15', 'NOUN'),\n",
                            "  ('\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23', 'PROPN'),\n",
                            "  ('\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07', 'VERB'),\n",
                            "  ('\u0e27\u0e48\u0e32', 'SCONJ'),\n",
                            "  ('<s/>', 'PUNCT')],\n",
                            " [('\u0e44\u0e14\u0e49', 'AUX'),\n",
                            "  ('\u0e19\u0e33', 'VERB'),\n",
                            "  ('\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28', 'NOUN'),\n",
                            "  ('\u0e40\u0e15\u0e37\u0e2d\u0e19', 'VERB'),\n",
                            "  ('\u0e1b\u0e25\u0e34\u0e07', 'NOUN'),\n",
                            "  ('\u0e44\u0e1b', 'VERB'),\n",
                            "  ('\u0e1b\u0e31\u0e01', 'VERB'),\n",
                            "  ('\u0e15\u0e32\u0e21', 'ADP'),\n",
                            "  ('\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33', 'NOUN'),\n",
                            "  (' \\n', 'NOUN'),\n",
                            "  ('\u0e43\u0e19', 'ADP'),\n",
                            "  ('\u0e40\u0e02\u0e15', 'NOUN'),\n",
                            "  ('\u0e2d\u0e33\u0e40\u0e20\u0e2d', 'NOUN'),\n",
                            "  ('\u0e40\u0e21\u0e37\u0e2d\u0e07', 'NOUN'),\n",
                            "  ('<s/>', 'PUNCT')],\n",
                            " [('\u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14', 'NOUN'),\n",
                            "  ('\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07', 'PROPN'),\n",
                            "  ('<s/>', 'PUNCT'),\n",
                            "  ('\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01', 'SCONJ'),\n",
                            "  ('\u0e19\u0e32\u0e22', 'NOUN'),\n",
                            "  ('\u0e2a\u0e38', 'PROPN'),\n",
                            "  ('\u0e01\u0e34\u0e08', 'NOUN'),\n",
                            "  ('<s/>', 'PUNCT')],\n",
                            " [('\u0e2d\u0e32\u0e22\u0e38', 'NOUN'), ('<s/>', 'PUNCT')],\n",
                            " [('65 ', 'NUM'), ('\u0e1b\u0e35', 'NOUN'), ('<s/>', 'PUNCT')],\n",
                            " [('\u0e16\u0e39\u0e01', 'AUX'),\n",
                            "  ('\u0e1b\u0e25\u0e34\u0e07', 'VERB'),\n",
                            "  ('\u0e01\u0e31\u0e14', 'VERB'),\n",
                            "  ('\u0e41\u0e25\u0e49\u0e27', 'ADV'),\n",
                            "  ('\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49', 'AUX'),\n",
                            "  ('\u0e44\u0e1b', 'VERB'),\n",
                            "  ('\u0e1e\u0e1a', 'VERB'),\n",
                            "  ('\u0e41\u0e1e\u0e17\u0e22\u0e4c', 'NOUN'),\n",
                            "  ('<s/>', 'PUNCT')]]"
                        ]
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import tltk\n",
                "\n",
                "phrase = \"\"\"\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19\u0e40\u0e02\u0e15\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07\u0e27\u0e48\u0e32 \u0e44\u0e14\u0e49\u0e19\u0e33\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28\u0e40\u0e15\u0e37\u0e2d\u0e19\u0e1b\u0e25\u0e34\u0e07\u0e44\u0e1b\u0e1b\u0e31\u0e01\u0e15\u0e32\u0e21\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33 \n",
                "\u0e43\u0e19\u0e40\u0e02\u0e15\u0e2d\u0e33\u0e40\u0e20\u0e2d\u0e40\u0e21\u0e37\u0e2d\u0e07 \u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07 \u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01\u0e19\u0e32\u0e22\u0e2a\u0e38\u0e01\u0e34\u0e08 \u0e2d\u0e32\u0e22\u0e38 65 \u0e1b\u0e35 \u0e16\u0e39\u0e01\u0e1b\u0e25\u0e34\u0e07\u0e01\u0e31\u0e14\u0e41\u0e25\u0e49\u0e27\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49\u0e44\u0e1b\u0e1e\u0e1a\u0e41\u0e1e\u0e17\u0e22\u0e4c\"\"\"\n",
                "\n",
                "pieces = tltk.nlp.pos_tag(phrase)\n",
                "pieces"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you just want to split out everything individually, you'll need to jump through a tiny hoop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19', 'NOUN'), ('\u0e40\u0e02\u0e15', 'NOUN'), ('\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23', 'PROPN'), ('\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07', 'VERB'), ('\u0e27\u0e48\u0e32', 'SCONJ'), ('<s/>', 'PUNCT'), ('\u0e44\u0e14\u0e49', 'AUX'), ('\u0e19\u0e33', 'VERB'), ('\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28', 'NOUN'), ('\u0e40\u0e15\u0e37\u0e2d\u0e19', 'VERB'), ('\u0e1b\u0e25\u0e34\u0e07', 'NOUN'), ('\u0e44\u0e1b', 'VERB'), ('\u0e1b\u0e31\u0e01', 'VERB'), ('\u0e15\u0e32\u0e21', 'ADP'), ('\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33', 'NOUN'), (' \\n', 'NOUN'), ('\u0e43\u0e19', 'ADP'), ('\u0e40\u0e02\u0e15', 'NOUN'), ('\u0e2d\u0e33\u0e40\u0e20\u0e2d', 'NOUN'), ('\u0e40\u0e21\u0e37\u0e2d\u0e07', 'NOUN'), ('<s/>', 'PUNCT'), ('\u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14', 'NOUN'), ('\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07', 'PROPN'), ('<s/>', 'PUNCT'), ('\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01', 'SCONJ'), ('\u0e19\u0e32\u0e22', 'NOUN'), ('\u0e2a\u0e38', 'PROPN'), ('\u0e01\u0e34\u0e08', 'NOUN'), ('<s/>', 'PUNCT'), ('\u0e2d\u0e32\u0e22\u0e38', 'NOUN'), ('<s/>', 'PUNCT'), ('65 ', 'NUM'), ('\u0e1b\u0e35', 'NOUN'), ('<s/>', 'PUNCT'), ('\u0e16\u0e39\u0e01', 'AUX'), ('\u0e1b\u0e25\u0e34\u0e07', 'VERB'), ('\u0e01\u0e31\u0e14', 'VERB'), ('\u0e41\u0e25\u0e49\u0e27', 'ADV'), ('\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49', 'AUX'), ('\u0e44\u0e1b', 'VERB'), ('\u0e1e\u0e1a', 'VERB'), ('\u0e41\u0e1e\u0e17\u0e22\u0e4c', 'NOUN'), ('<s/>', 'PUNCT')]\n"
                    ]
                }
            ],
            "source": [
                "words = [word for piece in pieces for word in piece]\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you'd like to cast away the part of speech, just ask for the first part of the pair."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['\u0e2a\u0e33\u0e19\u0e31\u0e01\u0e07\u0e32\u0e19', '\u0e40\u0e02\u0e15', '\u0e08\u0e15\u0e38\u0e08\u0e31\u0e01\u0e23', '\u0e0a\u0e35\u0e49\u0e41\u0e08\u0e07', '\u0e27\u0e48\u0e32', '<s/>', '\u0e44\u0e14\u0e49', '\u0e19\u0e33', '\u0e1b\u0e49\u0e32\u0e22\u0e1b\u0e23\u0e30\u0e01\u0e32\u0e28', '\u0e40\u0e15\u0e37\u0e2d\u0e19', '\u0e1b\u0e25\u0e34\u0e07', '\u0e44\u0e1b', '\u0e1b\u0e31\u0e01', '\u0e15\u0e32\u0e21', '\u0e41\u0e2b\u0e25\u0e48\u0e07\u0e19\u0e49\u0e33', ' \\n', '\u0e43\u0e19', '\u0e40\u0e02\u0e15', '\u0e2d\u0e33\u0e40\u0e20\u0e2d', '\u0e40\u0e21\u0e37\u0e2d\u0e07', '<s/>', '\u0e08\u0e31\u0e07\u0e2b\u0e27\u0e31\u0e14', '\u0e2d\u0e48\u0e32\u0e07\u0e17\u0e2d\u0e07', '<s/>', '\u0e2b\u0e25\u0e31\u0e07\u0e08\u0e32\u0e01', '\u0e19\u0e32\u0e22', '\u0e2a\u0e38', '\u0e01\u0e34\u0e08', '<s/>', '\u0e2d\u0e32\u0e22\u0e38', '<s/>', '65 ', '\u0e1b\u0e35', '<s/>', '\u0e16\u0e39\u0e01', '\u0e1b\u0e25\u0e34\u0e07', '\u0e01\u0e31\u0e14', '\u0e41\u0e25\u0e49\u0e27', '\u0e44\u0e21\u0e48\u0e44\u0e14\u0e49', '\u0e44\u0e1b', '\u0e1e\u0e1a', '\u0e41\u0e1e\u0e17\u0e22\u0e4c', '<s/>']\n"
                    ]
                }
            ],
            "source": [
                "words = [word[0] for piece in pieces for word in piece]\n",
                "print(words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Vietnamese: pyvi\n",
                "\n",
                "For Vietnamese we'll use [pyvi](https://github.com/trungtv/pyvi). There are [plenty of other options](https://github.com/undertheseanlp/NLP-Vietnamese-progress/blob/master/tasks/word_segmentation.md), but the best ones all involve installing Java and separate packages. We'll use pyvi to keep it simple."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install pyvi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Weirdly, when you run the `tokenize` method you get a string back..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Tr\u01b0\u1eddng \u0111\u1ea1i_h\u1ecdc b\u00e1ch_khoa h\u00e0_n\u1ed9i'"
                        ]
                    },
                    "execution_count": 62,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from pyvi import ViTokenizer, ViPosTagger\n",
                "\n",
                "words = ViTokenizer.tokenize(u\"Tr\u01b0\u1eddng \u0111\u1ea1i h\u1ecdc b\u00e1ch khoa h\u00e0 n\u1ed9i\")\n",
                "words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['Tr\u01b0\u1eddng', '\u0111\u1ea1i_h\u1ecdc', 'b\u00e1ch_khoa', 'h\u00e0_n\u1ed9i']"
                        ]
                    },
                    "execution_count": 63,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "words = words.split(\" \")\n",
                "words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "But! If you're also hunting for parts of speech, you end up with a list."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(['Tr\u01b0\u1eddng', '\u0111\u1ea1i_h\u1ecdc', 'B\u00e1ch_Khoa', 'H\u00e0_N\u1ed9i'], ['N', 'N', 'Np', 'Np'])"
                        ]
                    },
                    "execution_count": 64,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "ViPosTagger.postagging(ViTokenizer.tokenize(u\"Tr\u01b0\u1eddng \u0111\u1ea1i h\u1ecdc B\u00e1ch Khoa H\u00e0 N\u1ed9i\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can split the words and parts of speech apart easily enough, if you need them in separate variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "words are ['Tr\u01b0\u1eddng', '\u0111\u1ea1i_h\u1ecdc', 'B\u00e1ch_Khoa', 'H\u00e0_N\u1ed9i']\n",
                        "pos are ['N', 'N', 'Np', 'Np']\n"
                    ]
                }
            ],
            "source": [
                "words, pos = ViPosTagger.postagging(ViTokenizer.tokenize(u\"Tr\u01b0\u1eddng \u0111\u1ea1i h\u1ecdc B\u00e1ch Khoa H\u00e0 N\u1ed9i\"))\n",
                "print('words are', words)\n",
                "print('pos are', pos)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you'd like them matched up (like in some of the examples above), you can use `zip` to pair the word and the part of speech."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[('Tr\u01b0\u1eddng', 'N'), ('\u0111\u1ea1i_h\u1ecdc', 'N'), ('B\u00e1ch_Khoa', 'Np'), ('H\u00e0_N\u1ed9i', 'Np')]"
                        ]
                    },
                    "execution_count": 69,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "list(zip(words, pos))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Review\n",
                "\n",
                "In this section we looked at **tokenizing** text in several different languages that can't just be split with spaces. To discover how to use these libraries with scikit-learn vectorizers, check out tutorial on [how to make scikit-learn vectorizers work with Japanese, Chinese, and other East Asian languages page](/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": true,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}