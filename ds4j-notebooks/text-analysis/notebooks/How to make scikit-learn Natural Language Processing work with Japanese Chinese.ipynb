{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How to use NLP with scikit-learn vectorizers in Japanese, Chinese (and other East Asian languages) by using a custom tokenizer\n",
                "\n",
                "While it's easy to get scikit-learn to play nicely with Japanese, Chinese, and other East Asian languages, most documentation is based around processing English. In this section we'll use a few tricks to override sklearn's English-language focus."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<p class=\"reading-options\">\n  <a class=\"btn\" href=\"/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese\">\n    <i class=\"fa fa-sm fa-book\"></i>\n    Read online\n  </a>\n  <a class=\"btn\" href=\"/text-analysis/notebooks/How to make scikit-learn Natural Language Processing work with Japanese Chinese.ipynb\">\n    <i class=\"fa fa-sm fa-download\"></i>\n    Download notebook\n  </a>\n  <a class=\"btn\" href=\"https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/How to make scikit-learn Natural Language Processing work with Japanese Chinese.ipynb\" target=\"_new\">\n    <i class=\"fa fa-sm fa-laptop\"></i>\n    Interactive version\n  </a>\n</p>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The problem\n",
                "\n",
                "### Working in English\n",
                "\n",
                "When you use scikit-learn to do text analysis, the very first step is usually **splitting and counting words.** Let's take a simple example of a few English sentences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "texts = [\n",
                "    \"Penny bought bright blue fishes.\",\n",
                "    \"Penny bought bright blue and orange fish.\",\n",
                "    \"The cat ate a fish at the store.\",\n",
                "    \"Penny went to the store. Penny ate a bug. Penny saw a fish.\",\n",
                "    \"Penny is a fish\"\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we'll use scikit-learn's `CountVectorizer` to count the words in each sentence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "pd.set_option(\"display.max_columns\", 30)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>and</th>\n",
                            "      <th>at</th>\n",
                            "      <th>ate</th>\n",
                            "      <th>blue</th>\n",
                            "      <th>bought</th>\n",
                            "      <th>bright</th>\n",
                            "      <th>bug</th>\n",
                            "      <th>cat</th>\n",
                            "      <th>fish</th>\n",
                            "      <th>fishes</th>\n",
                            "      <th>is</th>\n",
                            "      <th>orange</th>\n",
                            "      <th>penny</th>\n",
                            "      <th>saw</th>\n",
                            "      <th>store</th>\n",
                            "      <th>the</th>\n",
                            "      <th>to</th>\n",
                            "      <th>went</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   and  at  ate  blue  bought  bright  bug  cat  fish  fishes  is  orange  \\\n",
                            "0    0   0    0     1       1       1    0    0     0       1   0       0   \n",
                            "1    1   0    0     1       1       1    0    0     1       0   0       1   \n",
                            "2    0   1    1     0       0       0    0    1     1       0   0       0   \n",
                            "3    0   0    1     0       0       0    1    0     1       0   0       0   \n",
                            "4    0   0    0     0       0       0    0    0     1       0   1       0   \n",
                            "\n",
                            "   penny  saw  store  the  to  went  \n",
                            "0      1    0      0    0   0     0  \n",
                            "1      1    0      0    0   0     0  \n",
                            "2      0    0      1    2   0     0  \n",
                            "3      3    1      1    1   1     1  \n",
                            "4      1    0      0    0   0     0  "
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "vectorizer = CountVectorizer()\n",
                "matrix = vectorizer.fit_transform(texts)\n",
                "\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Nice and easy, right?** Scikit-learn's `CountVectorizer` does a few steps:\n",
                "\n",
                "1. Separates the words\n",
                "2. Makes them all lowercase\n",
                "3. Finds all the unique words\n",
                "4. Counts the unique words\n",
                "5. Throws us a little party and makes us very happy\n",
                "\n",
                "If you need review of how all that works, I recommend you check out the [advanced word counting](/text-analysis/counting-words-with-scikit-learns-countvectorizer/) and [TF-IDF](/text-analysis/counting-words-with-scikit-learns-countvectorizer/) explanations.\n",
                "\n",
                "The problem shows up, though, when we **try to use Japanese**.\n",
                "\n",
                "### Working in Japanese\n",
                "\n",
                "Let's try the same thing we did above, but using Japanese."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "texts_jp = [\n",
                "    \"\u30da\u30cb\u30fc\u306f\u9bae\u3084\u304b\u306a\u9752\u3044\u9b5a\u3092\u8cb7\u3063\u305f\u3002\",\n",
                "    \"\u30da\u30cb\u30fc\u306f\u660e\u308b\u3044\u9752\u3068\u30aa\u30ec\u30f3\u30b8\u306e\u9b5a\u3092\u8cb7\u3063\u305f\u3002\",\n",
                "    \"\u732b\u306f\u5e97\u3067\u9b5a\u3092\u98df\u3079\u307e\u3057\u305f\u3002\",\n",
                "    \"\u30da\u30cb\u30fc\u306f\u5e97\u306b\u884c\u304d\u307e\u3057\u305f\u3002\u30da\u30cb\u30fc\u306f\u866b\u3092\u98df\u3079\u307e\u3057\u305f\u3002\u30da\u30cb\u30fc\u306f\u9b5a\u3092\u898b\u307e\u3057\u305f\u3002\",\n",
                "    \"\u30da\u30cb\u30fc\u306f\u9b5a\u3067\u3059\"\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>\u30da\u30cb\u30fc\u306f\u5e97\u306b\u884c\u304d\u307e\u3057\u305f</th>\n",
                            "      <th>\u30da\u30cb\u30fc\u306f\u660e\u308b\u3044\u9752\u3068\u30aa\u30ec\u30f3\u30b8\u306e\u9b5a\u3092\u8cb7\u3063\u305f</th>\n",
                            "      <th>\u30da\u30cb\u30fc\u306f\u866b\u3092\u98df\u3079\u307e\u3057\u305f</th>\n",
                            "      <th>\u30da\u30cb\u30fc\u306f\u9b5a\u3067\u3059</th>\n",
                            "      <th>\u30da\u30cb\u30fc\u306f\u9b5a\u3092\u898b\u307e\u3057\u305f</th>\n",
                            "      <th>\u30da\u30cb\u30fc\u306f\u9bae\u3084\u304b\u306a\u9752\u3044\u9b5a\u3092\u8cb7\u3063\u305f</th>\n",
                            "      <th>\u732b\u306f\u5e97\u3067\u9b5a\u3092\u98df\u3079\u307e\u3057\u305f</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   \u30da\u30cb\u30fc\u306f\u5e97\u306b\u884c\u304d\u307e\u3057\u305f  \u30da\u30cb\u30fc\u306f\u660e\u308b\u3044\u9752\u3068\u30aa\u30ec\u30f3\u30b8\u306e\u9b5a\u3092\u8cb7\u3063\u305f  \u30da\u30cb\u30fc\u306f\u866b\u3092\u98df\u3079\u307e\u3057\u305f  \u30da\u30cb\u30fc\u306f\u9b5a\u3067\u3059  \u30da\u30cb\u30fc\u306f\u9b5a\u3092\u898b\u307e\u3057\u305f  \\\n",
                            "0            0                    0            0        0           0   \n",
                            "1            0                    1            0        0           0   \n",
                            "2            0                    0            0        0           0   \n",
                            "3            1                    0            1        0           1   \n",
                            "4            0                    0            0        1           0   \n",
                            "\n",
                            "   \u30da\u30cb\u30fc\u306f\u9bae\u3084\u304b\u306a\u9752\u3044\u9b5a\u3092\u8cb7\u3063\u305f  \u732b\u306f\u5e97\u3067\u9b5a\u3092\u98df\u3079\u307e\u3057\u305f  \n",
                            "0                1            0  \n",
                            "1                0            0  \n",
                            "2                0            1  \n",
                            "3                0            0  \n",
                            "4                0            0  "
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "vectorizer = CountVectorizer()\n",
                "matrix = vectorizer.fit_transform(texts_jp)\n",
                "\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Oof, ouch, ow! **That's terrible!**\n",
                "\n",
                "Because scikit-learn's vectorizer doesn't know how to split the Japanese sentences apart (also known as **segmentation**), it just tries to separate them based on spaces. Since Japanese doesn't use spaces, we end up with **each sentence being considered a single word!**\n",
                "\n",
                "## Segmenting in non-English languages\n",
                "\n",
                "There's another page where we [learned to split words in East Asian languages](/text-analysis/splitting-words-in-east-asian-languages/), and it wasn't bad at all. Let's see how it works for an example in Japanese, using the [nagisa](https://github.com/taishi-i/nagisa) library.\n",
                "\n",
                "> If you're interested in another language, keep reading! The same concepts apply to Chinese, Vietnamese, etc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\u30da\u30cb\u30fc', '\u306f', '\u9bae\u3084\u304b', '\u306a', '\u9752\u3044', '\u9b5a', '\u3092', '\u8cb7\u3063', '\u305f', '\u3002']"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import nagisa\n",
                "\n",
                "text = '\u30da\u30cb\u30fc\u306f\u9bae\u3084\u304b\u306a\u9752\u3044\u9b5a\u3092\u8cb7\u3063\u305f\u3002'\n",
                "doc = nagisa.tagging(text)\n",
                "\n",
                "doc.words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "While that's nice and fun and cool and wonderful, **it doesn't actually help us with our machine learning.** All of the machine learning on this site is based on scikit-learn, where the `CountVectorizer` or `TfidfVectorizer` splits the text for us, *not* some extra library.\n",
                "\n",
                "So how do we teach scikit-learn to use nagisa?\n",
                "\n",
                "## Using custom text segmentation in scikit-learn\n",
                "\n",
                "We have a few options when teaching scikit-learn's vectorizers segment Japanese, Chinese, or other East Asian languages. The easiest technique is to give it a **custom tokenizer**.\n",
                "\n",
                "Tokenization is the process of splitting words apart. If we can replace the vectorizer's default English-language tokenizer with the nagisa tokenizer, we'll be all set!\n",
                "\n",
                "The first thing we need to do is **write a function that will tokenize a sentence.** Since we'll be tokenizing Japanese, we'll call it `tokenize_jp`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Takes in a document, returns the list of words\n",
                "def tokenize_jp(doc):\n",
                "    doc = nagisa.tagging(doc)\n",
                "    return doc.words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['\u30da\u30cb\u30fc', '\u306f', '\u9bae\u3084\u304b', '\u306a', '\u9752\u3044', '\u9b5a', '\u3092', '\u8cb7\u3063', '\u305f', '\u3002']\n",
                        "['\u732b', '\u306f', '\u5e97', '\u3067', '\u9b5a', '\u3092', '\u98df\u3079', '\u307e\u3057', '\u305f', '\u3002']\n"
                    ]
                }
            ],
            "source": [
                "# Test it out\n",
                "\n",
                "print(tokenize_jp(\"\u30da\u30cb\u30fc\u306f\u9bae\u3084\u304b\u306a\u9752\u3044\u9b5a\u3092\u8cb7\u3063\u305f\u3002\"))\n",
                "print(tokenize_jp(\"\u732b\u306f\u5e97\u3067\u9b5a\u3092\u98df\u3079\u307e\u3057\u305f\u3002\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now all we need to do is **tell our vectorizer to use our custom tokenizer.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>\u3002</th>\n",
                            "      <th>\u305f</th>\n",
                            "      <th>\u3067</th>\n",
                            "      <th>\u3067\u3059</th>\n",
                            "      <th>\u3068</th>\n",
                            "      <th>\u306a</th>\n",
                            "      <th>\u306b</th>\n",
                            "      <th>\u306e</th>\n",
                            "      <th>\u306f</th>\n",
                            "      <th>\u307e\u3057</th>\n",
                            "      <th>...</th>\n",
                            "      <th>\u732b</th>\n",
                            "      <th>\u866b</th>\n",
                            "      <th>\u884c\u304d</th>\n",
                            "      <th>\u898b</th>\n",
                            "      <th>\u8cb7\u3063</th>\n",
                            "      <th>\u9752</th>\n",
                            "      <th>\u9752\u3044</th>\n",
                            "      <th>\u98df\u3079</th>\n",
                            "      <th>\u9b5a</th>\n",
                            "      <th>\u9bae\u3084\u304b</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>3</td>\n",
                            "      <td>3</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>3</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>5 rows \u00d7 25 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   \u3002  \u305f  \u3067  \u3067\u3059  \u3068  \u306a  \u306b  \u306e  \u306f  \u307e\u3057  ...  \u732b  \u866b  \u884c\u304d  \u898b  \u8cb7\u3063  \u9752  \u9752\u3044  \u98df\u3079  \u9b5a  \u9bae\u3084\u304b\n",
                            "0  1  1  0   0  0  1  0  0  1   0  ...  0  0   0  0   1  0   1   0  1    1\n",
                            "1  1  1  0   0  1  0  0  1  1   0  ...  0  0   0  0   1  1   0   0  1    0\n",
                            "2  1  1  1   0  0  0  0  0  1   1  ...  1  0   0  0   0  0   0   1  1    0\n",
                            "3  3  3  0   0  0  0  1  0  3   3  ...  0  1   1  1   0  0   0   1  1    0\n",
                            "4  0  0  0   1  0  0  0  0  1   0  ...  0  0   0  0   0  0   0   0  1    0\n",
                            "\n",
                            "[5 rows x 25 columns]"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "vectorizer = CountVectorizer(tokenizer=tokenize_jp)\n",
                "matrix = vectorizer.fit_transform(texts_jp)\n",
                "\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tada! **It's like magic!**\n",
                "\n",
                "Since we're only overriding the tokenizer, we can also do things like use **n-grams** or **custom stopword lists** without any trouble."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>\u3067\u3059</th>\n",
                            "      <th>\u306b</th>\n",
                            "      <th>\u306b \u884c\u304d</th>\n",
                            "      <th>\u306e</th>\n",
                            "      <th>\u306e \u9b5a</th>\n",
                            "      <th>\u307e\u3057</th>\n",
                            "      <th>\u307e\u3057 \u30da\u30cb\u30fc</th>\n",
                            "      <th>\u3092</th>\n",
                            "      <th>\u3092 \u898b</th>\n",
                            "      <th>\u3092 \u8cb7\u3063</th>\n",
                            "      <th>...</th>\n",
                            "      <th>\u9752 \u30aa\u30ec\u30f3\u30b8</th>\n",
                            "      <th>\u9752\u3044</th>\n",
                            "      <th>\u9752\u3044 \u9b5a</th>\n",
                            "      <th>\u98df\u3079</th>\n",
                            "      <th>\u98df\u3079 \u307e\u3057</th>\n",
                            "      <th>\u9b5a</th>\n",
                            "      <th>\u9b5a \u3067\u3059</th>\n",
                            "      <th>\u9b5a \u3092</th>\n",
                            "      <th>\u9bae\u3084\u304b</th>\n",
                            "      <th>\u9bae\u3084\u304b \u9752\u3044</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>2</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>...</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>5 rows \u00d7 44 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   \u3067\u3059  \u306b  \u306b \u884c\u304d  \u306e  \u306e \u9b5a  \u307e\u3057  \u307e\u3057 \u30da\u30cb\u30fc  \u3092  \u3092 \u898b  \u3092 \u8cb7\u3063  ...  \u9752 \u30aa\u30ec\u30f3\u30b8  \u9752\u3044  \u9752\u3044 \u9b5a  \u98df\u3079  \\\n",
                            "0   0  0     0  0    0   0       0  1    0     1  ...       0   1     1   0   \n",
                            "1   0  0     0  1    1   0       0  1    0     1  ...       1   0     0   0   \n",
                            "2   0  0     0  0    0   1       0  1    0     0  ...       0   0     0   1   \n",
                            "3   0  1     1  0    0   3       2  2    1     0  ...       0   0     0   1   \n",
                            "4   1  0     0  0    0   0       0  0    0     0  ...       0   0     0   0   \n",
                            "\n",
                            "   \u98df\u3079 \u307e\u3057  \u9b5a  \u9b5a \u3067\u3059  \u9b5a \u3092  \u9bae\u3084\u304b  \u9bae\u3084\u304b \u9752\u3044  \n",
                            "0      0  1     0    1    1       1  \n",
                            "1      0  1     0    1    0       0  \n",
                            "2      1  1     0    1    0       0  \n",
                            "3      1  1     0    1    0       0  \n",
                            "4      0  1     1    0    0       0  \n",
                            "\n",
                            "[5 rows x 44 columns]"
                        ]
                    },
                    "execution_count": 46,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "stop_words = ['\u3002', '\u306a', '\u3068', '\u305f', '\u3067', '\u306f']\n",
                "vectorizer = CountVectorizer(tokenizer=tokenize_jp, ngram_range=(1,2), stop_words=stop_words)\n",
                "matrix = vectorizer.fit_transform(texts_jp)\n",
                "\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Customizing our Japanese tokenizer further\n",
                "\n",
                "Custom stopword lists are nice, but I don't want to type out things like `\u3002` and `\u306f`, I just want to say \"please don't include punctuation or particles.\" It turns out [this is possible with nagisa](https://github.com/taishi-i/nagisa#post-processing-functions), as in their example:\n",
                "\n",
                "```python\n",
                "text = 'Python\u3067\u7c21\u5358\u306b\u4f7f\u3048\u308b\u30c4\u30fc\u30eb\u3067\u3059'\n",
                "# Filter the words of the specific POS tags.\n",
                "words = nagisa.filter(text, filter_postags=['\u52a9\u8a5e', '\u52a9\u52d5\u8a5e'])\n",
                "print(words)\n",
                "#=> Python/\u540d\u8a5e \u7c21\u5358/\u5f62\u72b6\u8a5e \u4f7f\u3048\u308b/\u52d5\u8a5e \u30c4\u30fc\u30eb/\u540d\u8a5e\n",
                "```\n",
                "\n",
                "We can do the same thing by **adapting this code to our tokenizer**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>\u30aa\u30ec\u30f3\u30b8</th>\n",
                            "      <th>\u30da\u30cb\u30fc</th>\n",
                            "      <th>\u5e97</th>\n",
                            "      <th>\u660e\u308b\u3044</th>\n",
                            "      <th>\u732b</th>\n",
                            "      <th>\u866b</th>\n",
                            "      <th>\u884c\u304d</th>\n",
                            "      <th>\u898b</th>\n",
                            "      <th>\u8cb7\u3063</th>\n",
                            "      <th>\u9752</th>\n",
                            "      <th>\u9752\u3044</th>\n",
                            "      <th>\u98df\u3079</th>\n",
                            "      <th>\u9b5a</th>\n",
                            "      <th>\u9bae\u3084\u304b</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0</td>\n",
                            "      <td>3</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   \u30aa\u30ec\u30f3\u30b8  \u30da\u30cb\u30fc  \u5e97  \u660e\u308b\u3044  \u732b  \u866b  \u884c\u304d  \u898b  \u8cb7\u3063  \u9752  \u9752\u3044  \u98df\u3079  \u9b5a  \u9bae\u3084\u304b\n",
                            "0     0    1  0    0  0  0   0  0   1  0   1   0  1    1\n",
                            "1     1    1  0    1  0  0   0  0   1  1   0   0  1    0\n",
                            "2     0    0  1    0  1  0   0  0   0  0   0   1  1    0\n",
                            "3     0    3  1    0  0  1   1  1   0  0   0   1  1    0\n",
                            "4     0    1  0    0  0  0   0  0   0  0   0   0  1    0"
                        ]
                    },
                    "execution_count": 53,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Takes in a document, filtering out particles, punctuation, and verb endings\n",
                "def tokenize_jp(text):\n",
                "    doc = nagisa.filter(text, filter_postags=['\u52a9\u8a5e', '\u88dc\u52a9\u8a18\u53f7', '\u52a9\u52d5\u8a5e'])\n",
                "    return doc.words\n",
                "\n",
                "vectorizer = CountVectorizer(tokenizer=tokenize_jp)\n",
                "matrix = vectorizer.fit_transform(texts_jp)\n",
                "\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Using a TF-IDF vectorizer with Chinese or Japanese\n",
                "\n",
                "For most vectorizing, we're going to use a `TfidfVectorizer` instead of a `CountVectorizer`. In this example we'll override a `TfidfVectorizer`'s tokenizer in the same way that we did for the `CountVectorizer`. In this case, though, we'll be telling scikit-learn to use a Chinese tokenizer ([jieba, see details here](/text-analysis/splitting-words-in-east-asian-languages/#Chinese:-jieba)) instead of a Japanese tokenizer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": [
                "texts_zh = [\n",
                "  '\u7fe0\u82b1\u4e70\u4e86\u6d45\u84dd\u8272\u7684\u9c7c',\n",
                "  '\u7fe0\u82b1\u4e70\u4e86\u6d45\u84dd\u6a59\u8272\u7684\u9c7c',\n",
                "  '\u732b\u5728\u5546\u5e97\u5403\u4e86\u4e00\u6761\u9c7c',\n",
                "  '\u7fe0\u82b1\u53bb\u4e86\u5546\u5e97\u3002\u7fe0\u82b1\u4e70\u4e86\u4e00\u53ea\u866b\u5b50\u3002\u7fe0\u82b1\u770b\u5230\u4e00\u6761\u9c7c',\n",
                "  '\u7fe0\u82b1\u662f\u9c7c'  \n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['\u7fe0\u82b1', '\u4e70', '\u4e86', '\u6d45\u84dd\u8272', '\u7684', '\u9c7c']"
                        ]
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Demo how jieba works\n",
                "import jieba\n",
                "\n",
                "jieba.lcut('\u7fe0\u82b1\u4e70\u4e86\u6d45\u84dd\u8272\u7684\u9c7c')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "All we do is write a function that uses jieba as a custom tokenizer, and we're all set!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>\u4e00\u53ea</th>\n",
                            "      <th>\u4e00\u6761</th>\n",
                            "      <th>\u4e70</th>\n",
                            "      <th>\u4e86</th>\n",
                            "      <th>\u53bb</th>\n",
                            "      <th>\u5403</th>\n",
                            "      <th>\u5546\u5e97</th>\n",
                            "      <th>\u5728</th>\n",
                            "      <th>\u662f</th>\n",
                            "      <th>\u6a59\u8272</th>\n",
                            "      <th>\u6d45\u84dd</th>\n",
                            "      <th>\u6d45\u84dd\u8272</th>\n",
                            "      <th>\u732b</th>\n",
                            "      <th>\u7684</th>\n",
                            "      <th>\u770b\u5230</th>\n",
                            "      <th>\u7fe0\u82b1</th>\n",
                            "      <th>\u866b\u5b50</th>\n",
                            "      <th>\u9c7c</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>3</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   \u4e00\u53ea  \u4e00\u6761  \u4e70  \u4e86  \u53bb  \u5403  \u5546\u5e97  \u5728  \u662f  \u6a59\u8272  \u6d45\u84dd  \u6d45\u84dd\u8272  \u732b  \u7684  \u770b\u5230  \u7fe0\u82b1  \u866b\u5b50  \u9c7c\n",
                            "0   0   0  1  1  0  0   0  0  0   0   0    1  0  1   0   1   0  1\n",
                            "1   0   0  1  1  0  0   0  0  0   1   1    0  0  1   0   1   0  1\n",
                            "2   0   1  0  1  0  1   1  1  0   0   0    0  1  0   0   0   0  1\n",
                            "3   1   1  1  2  1  0   1  0  0   0   0    0  0  0   1   3   1  1\n",
                            "4   0   0  0  0  0  0   0  0  1   0   0    0  0  0   0   1   0  1"
                        ]
                    },
                    "execution_count": 68,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Takes in a document, separates the words\n",
                "def tokenize_zh(text):\n",
                "    words = jieba.lcut(text)\n",
                "    return words\n",
                "\n",
                "# Add a custom list of stopwords for punctuation\n",
                "stop_words = ['\u3002', '\uff0c']\n",
                "\n",
                "vectorizer = CountVectorizer(tokenizer=tokenize_zh, stop_words=stop_words)\n",
                "matrix = vectorizer.fit_transform(texts_zh)\n",
                "\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There we go!\n",
                "\n",
                "## What now?\n",
                "\n",
                "You can now do pretty much **everything this site uses a vectorizer for**, which is most of the natural language processing pieces. Just be careful to ignore anything when we talk about stemming or lemmatization - we have to write very very custom vectorizers to handle English being a horrible language, but you can avoid that and just use your `tokenizer=`.\n",
                "\n",
                "For example, in the [topic modeling section](/text-analysis/introduction-to-topic-modeling/) we build a vectorizer that looks like this:\n",
                "\n",
                "```python\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import Stemmer\n",
                "\n",
                "# English stemmer from pyStemmer\n",
                "stemmer = Stemmer.Stemmer('en')\n",
                "\n",
                "analyzer = TfidfVectorizer().build_analyzer()\n",
                "\n",
                "# Override TfidfVectorizer\n",
                "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
                "    def build_analyzer(self):\n",
                "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
                "        return lambda doc: stemmer.stemWords(analyzer(doc))\n",
                "\n",
                "# Vectorize and count words\n",
                "vectorizer = StemmedTfidfVectorizer(min_df=50)\n",
                "matrix = vectorizer.fit_transform(recipes.ingredient_list)\n",
                "\n",
                "# Get a nice readable dataframe of words\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df.head()\n",
                "```\n",
                "\n",
                "We have to jump through all of those steps because by default, scikit-learn vectorizers don't stem (`swim`, `swims`, `swimming` all turn into `swim`). As a result we need to do a LOT of overriding.\n",
                "\n",
                "**Languages like Chines or Japanese don't need stemming, though!** For example, if you were doing this in Japanese, you could skip all the complex parts and simply stick with a custom tokenizer:\n",
                "\n",
                "```python\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import nagisa\n",
                "\n",
                "# Takes in a document, filtering out particles, punctuation, and verb endings\n",
                "def tokenize_jp(text):\n",
                "    doc = nagisa.filter(text, filter_postags=['\u52a9\u8a5e', '\u88dc\u52a9\u8a18\u53f7', '\u52a9\u52d5\u8a5e'])\n",
                "    return doc.words\n",
                "\n",
                "# Vectorizer and count words (with a custom tokenizer)\n",
                "vectorizer = TfidfVectorizer(tokenizer=tokenize_jp, min_df=50)\n",
                "matrix = vectorizer.fit_transform(recipes.ingredient_list)\n",
                "\n",
                "# Get a nice readable dataframe of words\n",
                "words_df = pd.DataFrame(matrix.toarray(),\n",
                "                        columns=vectorizer.get_feature_names())\n",
                "words_df\n",
                "```\n",
                "\n",
                "Of course this won't work on that page because `recipes.ingredient_list` is in English, but hopefully you get the idea!\n",
                "\n",
                "## Review\n",
                "\n",
                "In this section, we learned how to use custom tokenizers to allow scikit-learn to play nicely with languages that don't use spaces to divide words. We specifically focused on building a Japanese vectorizer that used [nagisa](https://github.com/taishi-i/nagisa) as well as a Chinese one that used [jieba](https://github.com/fxsjy/jieba).\n",
                "\n",
                "For more on specifically Chinese TF-IDF, check [this page here](/text-analysis/using-tf-idf-with-chinese/). For segmenting words in other languages like Korean, Thai, or Vietnamese, visit [our East Asian word splitting page](/text-analysis/splitting-words-in-east-asian-languages/).\n",
                "\n",
                "## Discussion topics\n",
                "\n",
                "This is actually not a discussion topic, but a request: **if you find or make any NLP-based stories using non-English languages, please send them to me!** The only one we have so far is [this Caixin reproduction](/caixin-museum-word-count/counting-words-in-chinese-museum-names/) but I'd love to add more."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": true,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 3
}